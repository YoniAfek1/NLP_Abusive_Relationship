{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Y85S836wGiCJ",
      "metadata": {
        "id": "Y85S836wGiCJ"
      },
      "source": [
        "# **Introduction**\n",
        "* Abusive relationships are a critical issue that affect people worldwide.\n",
        "* With the rise of social media, more people are sharing personal stories.\n",
        "* Using NLP, we can detect signs of abuse and identify key risk factors.\n",
        "* Allows for efficient identification of abusive patterns and better support for victims."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rrlRaMDHHX7a",
      "metadata": {
        "id": "rrlRaMDHHX7a"
      },
      "source": [
        "# **Data**\n",
        "Dataset of 10k Reddit posts which has been processed to:\n",
        "* Post Metadata\n",
        "* Relationship and Demographic data generated by Gemini pro\n",
        "* Contexual Risk Factors generated by Gemini pro\n",
        "notice\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7FD2ZjtCh2VP",
      "metadata": {
        "id": "7FD2ZjtCh2VP"
      },
      "source": [
        "# **Installations & Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CCYQYskAOQXt",
      "metadata": {
        "id": "CCYQYskAOQXt"
      },
      "outputs": [],
      "source": [
        "# Installations\n",
        "!pip install matplotlib seaborn\n",
        "!pip install upgrade pandas\n",
        "!pip install openpyxl\n",
        "!pip install lime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SCN4ia5zc1kJ",
      "metadata": {
        "id": "SCN4ia5zc1kJ"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.colors import LogNorm\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, \\\n",
        "  precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "import seaborn as sns\n",
        "from tabulate import tabulate\n",
        "import statsmodels.api as sm\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import random\n",
        "\n",
        "from transformers import RobertaTokenizer, DistilBertTokenizer, RobertaForSequenceClassification, \\\n",
        "    DistilBertForSequenceClassification, AdamW\n",
        "\n",
        "import gc\n",
        "from lime.lime_text import LimeTextExplainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gYJheRoOIw2-",
      "metadata": {
        "id": "gYJheRoOIw2-"
      },
      "source": [
        "# **Part 1 - Data Validation**\n",
        "**Zoom out on the distributions of the data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KdnQCdlUs1xh",
      "metadata": {
        "id": "KdnQCdlUs1xh"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Load the Excel file, using the first row as headers\n",
        "df = pd.read_excel('Labeled.xlsx', sheet_name='All', engine='openpyxl')\n",
        "\n",
        "# Select the specified columns range\n",
        "columns_range = df.columns[23:70]\n",
        "\n",
        "# Define columns to exclude - columns with a different scale (shown in the next cell)\n",
        "exclude_columns = ['author_gender', 'age_female', 'age_male', 'author_role', 'relationship_type']\n",
        "\n",
        "# Filter out the columns to exclude\n",
        "selected_columns = [col for col in columns_range if col not in exclude_columns]\n",
        "\n",
        "# Create a new DataFrame with only the selected columns\n",
        "df_selected = df[selected_columns]\n",
        "\n",
        "# Define the possible labels\n",
        "labels = ['yes', 'plausibly', 'cannot be inferred', 'no', 'irrelevant']\n",
        "\n",
        "# Count the occurrences of each label in each column\n",
        "label_counts = pd.DataFrame({label: (df_selected == label).sum() for label in labels}, index=selected_columns)\n",
        "\n",
        "# Convert counts to percentages\n",
        "label_percentages = label_counts.div(label_counts.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Plot the stacked bar plot\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "label_percentages.plot(kind='barh', stacked=True, ax=ax, colormap='viridis')\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Percentage')\n",
        "ax.set_ylabel('Risk factor')\n",
        "ax.set_title(\"Distribution of risk factors\")\n",
        "\n",
        "# Adjust spacing and reverse the Y-axis order\n",
        "ax.set_yticks(range(len(selected_columns)))\n",
        "ax.set_yticklabels(selected_columns, fontsize=9)\n",
        "ax.invert_yaxis()\n",
        "\n",
        "# Show legend and adjust layout\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(left=0.3)  # Adjust margin for better readability\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ditribution of columns with different range**"
      ],
      "metadata": {
        "id": "XNPFjc8LekbL"
      },
      "id": "XNPFjc8LekbL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Excel file\n",
        "df_excluded = pd.read_excel('Labeled.xlsx', sheet_name='All', engine='openpyxl')\n",
        "\n",
        "# Define age bins and labels for grouping ages\n",
        "age_bins = [0, 20, 30, 40, 50, float('inf')]\n",
        "age_labels = ['<=20', '20-30', '30-40', '40-50', '>50']\n",
        "\n",
        "# Process age-related columns\n",
        "if 'age_female' in exclude_columns:\n",
        "    df_excluded['age_female_group'] = pd.cut(df_excluded['age_female'], bins=age_bins, labels=age_labels)\n",
        "if 'age_male' in exclude_columns:\n",
        "    df_excluded['age_male_group'] = pd.cut(df_excluded['age_male'], bins=age_bins, labels=age_labels)\n",
        "\n",
        "# Replace age columns with grouped versions in exclude_columns if present\n",
        "exclude_columns = [\n",
        "    'age_female_group' if 'age_female' in exclude_columns else 'age_female',\n",
        "    'age_male_group' if 'age_male' in exclude_columns else 'age_male',\n",
        "    *[col for col in exclude_columns if col not in ['age_female', 'age_male']]\n",
        "]\n",
        "\n",
        "# Iterate through each column in exclude_columns\n",
        "for column in exclude_columns:\n",
        "    # Skip columns that don't exist in the group\n",
        "    if column not in df_excluded:\n",
        "        continue\n",
        "\n",
        "    # Extract unique labels for the current column\n",
        "    labels = df_excluded[column].dropna().unique()\n",
        "\n",
        "    # Count occurrences of each label\n",
        "    label_counts = df_excluded[column].value_counts(normalize=True) * 100\n",
        "\n",
        "    # Plot the distribution of labels for the current column\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    label_counts.plot(kind='bar', ax=ax)  # Changed to 'bar' for vertical bars\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_xlabel('Labels')        # Switched x-axis label\n",
        "    ax.set_ylabel('Percentage')    # Switched y-axis label\n",
        "    ax.set_title(f\"Distribution of Labels in {column}\")\n",
        "\n",
        "    # Rotate x-axis labels if they are too long\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Adjust layout and display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "LvpoabzV_RiK"
      },
      "id": "LvpoabzV_RiK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that there are no missing values in the columns of Relationship, Demographic and risk factore, as expected, since the data was generated."
      ],
      "metadata": {
        "id": "E9ERtuCJh4zh"
      },
      "id": "E9ERtuCJh4zh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2 - Data Exploration**\n",
        "* Sampling 100 examples.\n",
        "* Verifying that the data is representative.\n",
        "* Manually classifying based on the stories."
      ],
      "metadata": {
        "id": "JtrQr7ugiAND"
      },
      "id": "JtrQr7ugiAND"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 We randomly selected 100 examples and examined their representation based on subreddits.**"
      ],
      "metadata": {
        "id": "7hPjiBKOi-Zs"
      },
      "id": "7hPjiBKOi-Zs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I9HpqsD-Dilo",
      "metadata": {
        "id": "I9HpqsD-Dilo"
      },
      "outputs": [],
      "source": [
        "# Define the sheets to process and the fixed row count for the sampled data\n",
        "sheets = ['Test', 'All']  # Test = sample, All = full dataset\n",
        "fixed_row_counts = {'Test': 100}  # Fixed row count for the sampled dataset\n",
        "\n",
        "# Dictionary to store extracted subreddit values from each sheet\n",
        "subreddit_data = {}\n",
        "\n",
        "# Load the Excel file once\n",
        "excel_file = pd.ExcelFile('Labeled.xlsx')\n",
        "\n",
        "# Process each sheet\n",
        "for sheet_name in sheets:\n",
        "    # Load the sheet into a DataFrame\n",
        "    df = excel_file.parse(sheet_name=sheet_name)\n",
        "\n",
        "    # Specify the columns to process (column at index 2)\n",
        "    subreddit_column = df.columns[2:3]\n",
        "\n",
        "    # Determine the number of rows to process\n",
        "    if sheet_name in fixed_row_counts:\n",
        "        num_rows = fixed_row_counts[sheet_name]\n",
        "    else:\n",
        "        num_rows = len(df)  # Dynamically calculate for the full dataset\n",
        "\n",
        "    # Extract subreddit values from the specified column\n",
        "    subreddit_values = []\n",
        "    for i in range(num_rows):\n",
        "        subreddit_values.extend(df.loc[i, subreddit_column].tolist())\n",
        "\n",
        "    # Store the subreddit values in the dictionary\n",
        "    subreddit_data[sheet_name] = subreddit_values\n",
        "\n",
        "# Access extracted subreddit values\n",
        "sampled_subreddits = subreddit_data['Test']\n",
        "full_subreddits = subreddit_data['All']\n",
        "\n",
        "# Count the occurrences of each unique subreddit\n",
        "sampled_counts = Counter(sampled_subreddits)\n",
        "full_counts = Counter(full_subreddits)\n",
        "\n",
        "# Combine \"survivinginfidelity\" into the key \"Infidelity\" for consistency\n",
        "sampled_counts['Infidelity'] += sampled_counts.pop('survivinginfidelity', 0)\n",
        "full_counts['Infidelity'] += full_counts.pop('survivinginfidelity', 0)\n",
        "\n",
        "# Calculate the total counts for normalization\n",
        "total_sampled = sum(sampled_counts.values())\n",
        "total_full = sum(full_counts.values())\n",
        "\n",
        "# Convert counts to percentages\n",
        "sampled_percentages = {subreddit: (count / total_sampled) * 100 for subreddit, count in sampled_counts.items()}\n",
        "full_percentages = {subreddit: (count / total_full) * 100 for subreddit, count in full_counts.items()}\n",
        "\n",
        "# Get a combined list of all unique subreddits from both datasets\n",
        "all_subreddits = set(sampled_percentages.keys()).union(set(full_percentages.keys()))\n",
        "\n",
        "# Sort subreddits by their percentage in the full dataset (descending order)\n",
        "sorted_subreddits = sorted(all_subreddits, key=lambda sub: full_percentages.get(sub, 0), reverse=True)\n",
        "\n",
        "# Prepare data for the grouped bar chart\n",
        "sampled_percentages_sorted = [sampled_percentages.get(sub, 0) for sub in sorted_subreddits]\n",
        "full_percentages_sorted = [full_percentages.get(sub, 0) for sub in sorted_subreddits]\n",
        "\n",
        "# Plot the grouped bar chart\n",
        "x_positions = range(len(sorted_subreddits))\n",
        "bar_width = 0.35  # Width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "bars_sampled = ax.bar([pos - bar_width/2 for pos in x_positions], sampled_percentages_sorted, bar_width, label='Sampled Data', color='skyblue')\n",
        "bars_full = ax.bar([pos + bar_width/2 for pos in x_positions], full_percentages_sorted, bar_width, label='Full Data', color='lightgreen')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Subreddit')\n",
        "ax.set_ylabel('Percentage')\n",
        "ax.set_title('Comparison of Subreddit Percentages: Sampled vs Full Data')\n",
        "ax.set_xticks(x_positions)\n",
        "ax.set_xticklabels(sorted_subreddits, rotation=90)\n",
        "ax.legend()\n",
        "\n",
        "# Annotate the bars with the percentage values\n",
        "for bars in [bars_sampled, bars_full]:\n",
        "    for bar in bars:\n",
        "        y_value = bar.get_height()\n",
        "        if y_value > 0:  # Only annotate bars with a value greater than 0\n",
        "            ax.text(\n",
        "                bar.get_x() + bar.get_width() / 2, y_value / 2, f'{round(y_value)}%',\n",
        "                ha='center', va='center', color='black'\n",
        "            )\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout(pad=3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 We compared Gemini Pro predictions with the true labels manually annotated by us.**"
      ],
      "metadata": {
        "id": "DeHfFtrrvIGC"
      },
      "id": "DeHfFtrrvIGC"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "# Load the Excel file\n",
        "df = pd.read_excel('Labeled.xlsx', sheet_name='Test', engine='openpyxl')\n",
        "\n",
        "# Define the contexts to search for in the column headers\n",
        "contexts_to_find = [\"Overall\", \"mental_condition\", \"jealousy\", \"emotional_violence\"]\n",
        "\n",
        "# Dynamically find column ranges or indices for the contexts\n",
        "contexts = {}\n",
        "for context in contexts_to_find:\n",
        "    if context == \"Overall\":\n",
        "        contexts[context] = df.columns[23:70]  # Overall is a range of columns\n",
        "    else:\n",
        "        contexts[context] = [col for col in df.columns if context in col]\n",
        "\n",
        "# Define number of rows for predicted and true values\n",
        "num_rows = 100\n",
        "\n",
        "# Define the labels for the confusion matrix\n",
        "labels = ['yes', 'plausibly', 'cannot be inferred', 'no']\n",
        "\n",
        "# Define colors for each plot\n",
        "plot_colors = {\n",
        "    \"Overall\": \"Blues\",\n",
        "    \"mental_condition\": \"Greens\",\n",
        "    \"jealousy\": \"Purples\",\n",
        "    \"emotional_violence\": \"Oranges\"\n",
        "}\n",
        "\n",
        "# Loop through each context and generate a plot\n",
        "for context_name, columns_range in contexts.items():\n",
        "    # Extract predicted values (first 100 rows)\n",
        "    Predicted_values = []\n",
        "    for i in range(num_rows):\n",
        "        Predicted_values.extend(df.loc[i, columns_range].tolist())\n",
        "\n",
        "    # Extract true values (rows 110-209)\n",
        "    True_values = []\n",
        "    for i in range(num_rows):\n",
        "        True_values.extend(df.loc[i + 109, columns_range].tolist())\n",
        "\n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix(Predicted_values, True_values, labels=labels)\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plot_colors[context_name], norm=LogNorm(vmin=1, vmax=cm.max()))\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Annotate the cells with counts\n",
        "    for i in range(len(labels)):\n",
        "        for j in range(len(labels)):\n",
        "            plt.text(j, i, cm[i, j], ha='center', va='center', color='black', fontweight='bold')\n",
        "\n",
        "    # Add gridlines\n",
        "    for i in range(len(labels) - 1):\n",
        "        plt.axhline(i + 0.5, color='black', linestyle='-', linewidth=1)\n",
        "        plt.axvline(i + 0.5, color='black', linestyle='-', linewidth=1)\n",
        "\n",
        "    # Add title and axis labels\n",
        "    plt.title(f'Confusion Matrix: {context_name}')\n",
        "    plt.xlabel('True Labels')\n",
        "    plt.ylabel('Predicted Labels')\n",
        "    plt.xticks(ticks=np.arange(len(labels)), labels=labels)\n",
        "    plt.yticks(ticks=np.arange(len(labels)), labels=labels)\n",
        "\n",
        "    # Adjust layout and show the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and print F1 scores\n",
        "    f1_weighted = f1_score(True_values, Predicted_values, average='weighted')\n",
        "    f1_macro = f1_score(True_values, Predicted_values, average='macro')\n",
        "    print(f\"F1 Score (Weighted) for {context_name}: {f1_weighted:.2f}\")\n",
        "    print(f\"F1 Score (Macro) for {context_name}: {f1_macro:.2f}\")\n"
      ],
      "metadata": {
        "id": "mc_JJFDMuY9Q"
      },
      "id": "mc_JJFDMuY9Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the comparison, we evaluated two metrics: weighted F1 and unweighted F1. The blue graph shows a general trend, while the following graphs are examples where the metrics evaluate similarly or differently."
      ],
      "metadata": {
        "id": "QA6CojEnwAFm"
      },
      "id": "QA6CojEnwAFm"
    },
    {
      "cell_type": "markdown",
      "id": "JHNhl1PksXZ0",
      "metadata": {
        "id": "JHNhl1PksXZ0"
      },
      "source": [
        "# **Part 3 - Data Preprocessing for models’ prediction**\n",
        "\n",
        "\n",
        "\n",
        "* Comparing differences in evaluation metrics.\n",
        "* Merging label categories into binary classifications.\n",
        "* Selecting the most reliable risk factors for the following parts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Comparison between weighted and unweighted F1**"
      ],
      "metadata": {
        "id": "j3CuNqPrz9zn"
      },
      "id": "j3CuNqPrz9zn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W7S0UkjMrSBW",
      "metadata": {
        "id": "W7S0UkjMrSBW"
      },
      "outputs": [],
      "source": [
        "# Read the Excel file\n",
        "df = pd.read_excel('Labeled.xlsx', sheet_name='Test')\n",
        "\n",
        "# List to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate over the column ranges from index 23 to 69\n",
        "for j in range(47):\n",
        "    column_name = df.columns[23 + j]\n",
        "    if column_name in ['age_female', 'age_male', 'author_gender']:\n",
        "        continue\n",
        "\n",
        "    num_rows = 100\n",
        "    Predicted_values = df.iloc[:num_rows][column_name].tolist()\n",
        "    True_values = df.iloc[109:109 + num_rows][column_name].tolist()\n",
        "\n",
        "    # Calculate the F1 scores\n",
        "    f1_weighted = f1_score(True_values, Predicted_values, average='weighted')\n",
        "    f1_macro = f1_score(True_values, Predicted_values, average='macro')\n",
        "\n",
        "    # Append the results to the list\n",
        "    results.append({\n",
        "        'Risk Factor': column_name,\n",
        "        'F1 Score - weighted': round(f1_weighted, 2),\n",
        "        'F1 Score - macro': round(f1_macro, 2)\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Sort the DataFrame by F1 Score - weighted in descending order\n",
        "results_df = results_df.sort_values(by='F1 Score - weighted', ascending=False)\n",
        "\n",
        "# Split the DataFrame into two parts\n",
        "midpoint = len(results_df) // 2\n",
        "df1 = results_df.iloc[:midpoint]\n",
        "df2 = results_df.iloc[midpoint:]\n",
        "\n",
        "# Display the two tables side by side with tabulate\n",
        "print(\"Table 1:\")\n",
        "print(tabulate(df1, headers='keys', tablefmt='pretty', showindex=False))\n",
        "print(\"\\nTable 2:\")\n",
        "print(tabulate(df2, headers='keys', tablefmt='pretty', showindex=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 Comparison of unweighted F1 (macro) before and after merging labels**"
      ],
      "metadata": {
        "id": "slgUN60r8fgv"
      },
      "id": "slgUN60r8fgv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RhLPtOj3SpGc",
      "metadata": {
        "id": "RhLPtOj3SpGc"
      },
      "outputs": [],
      "source": [
        "# Read the Excel file\n",
        "df = pd.read_excel('Labeled.xlsx', sheet_name='Test')\n",
        "\n",
        "# First table: F1 Score - weighted and F1 Score - macro\n",
        "results = []\n",
        "\n",
        "# Iterate over the column ranges from index 23 to 69\n",
        "for j in range(47):\n",
        "    column_name = df.columns[23 + j]\n",
        "    if column_name in ['age_female', 'age_male', 'author_gender']:\n",
        "        continue\n",
        "\n",
        "    num_rows = 100\n",
        "    Predicted_values = df.iloc[:num_rows][column_name].tolist()\n",
        "    True_values = df.iloc[109:109 + num_rows][column_name].tolist()\n",
        "\n",
        "    # Calculate the F1 scores\n",
        "    f1_weighted = f1_score(True_values, Predicted_values, average='weighted')\n",
        "    f1_macro = f1_score(True_values, Predicted_values, average='macro')\n",
        "\n",
        "    # Append the results to the list\n",
        "    results.append({\n",
        "        'Risk Factor': column_name,\n",
        "        'F1 Score - weighted': round(f1_weighted, 2),\n",
        "        'F1 Score - macro old': round(f1_macro, 2)\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Second table: F1 Score - macro after replacing labels\n",
        "columns_range = df.columns[34:70]\n",
        "num_rows_replace = 211\n",
        "df.loc[:num_rows_replace-1, columns_range] = df.loc[:num_rows_replace-1, columns_range].replace({'no': 'cannot be inferred', 'yes': 'plausibly'})\n",
        "\n",
        "new_results = []\n",
        "\n",
        "# Recalculate F1 scores after label replacement\n",
        "for j in range(47):\n",
        "    column_name = df.columns[23 + j]\n",
        "    if column_name in ['age_female', 'age_male', 'author_gender']:\n",
        "        continue\n",
        "\n",
        "    num_rows = 100\n",
        "    Predicted_values_new = df.iloc[:num_rows][column_name].tolist()\n",
        "    True_values_new = df.iloc[109:109 + num_rows][column_name].tolist()\n",
        "\n",
        "    # Calculate the new F1 score - macro\n",
        "    f1_macro_new = f1_score(True_values_new, Predicted_values_new, average='macro')\n",
        "\n",
        "    # Append the updated results to the list\n",
        "    new_results.append({\n",
        "        'Risk Factor': column_name,\n",
        "        'F1 Score - macro new': round(f1_macro_new, 2)\n",
        "    })\n",
        "\n",
        "# Create DataFrame from the new results\n",
        "results_new_df = pd.DataFrame(new_results)\n",
        "\n",
        "# Merge the old and new DataFrames on 'Risk Factor'\n",
        "final_df = pd.merge(results_df[['Risk Factor', 'F1 Score - macro old']], results_new_df, on='Risk Factor')\n",
        "\n",
        "# Rename the columns\n",
        "final_df.rename(columns={\n",
        "    'F1 Score - macro old': 'F1 - old',\n",
        "    'F1 Score - macro new': 'F1 - new'\n",
        "}, inplace=True)\n",
        "\n",
        "# Calculate the difference with two decimal places\n",
        "final_df['Difference'] = (final_df['F1 - new'] - final_df['F1 - old']).round(2)\n",
        "\n",
        "# Sort the final DataFrame by 'F1 - new'\n",
        "final_df = final_df.sort_values(by='F1 - new', ascending=False)\n",
        "\n",
        "# Reorder columns: F1 - new first, then F1 - old, then Difference\n",
        "final_df = final_df[['Risk Factor', 'F1 - new', 'F1 - old', 'Difference']]\n",
        "\n",
        "# Split the DataFrame into two parts\n",
        "midpoint = len(final_df) // 2\n",
        "df1 = final_df.iloc[:midpoint].reset_index(drop=True)\n",
        "df2 = final_df.iloc[midpoint:].reset_index(drop=True)\n",
        "\n",
        "# Display the two tables\n",
        "print(\"Table 1:\")\n",
        "print(tabulate(df1, headers='keys', tablefmt='pretty', showindex=False))\n",
        "print(\"\\nTable 2:\")\n",
        "print(tabulate(df2, headers='keys', tablefmt='pretty', showindex=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DQ_UWr9sozGn",
      "metadata": {
        "id": "DQ_UWr9sozGn"
      },
      "source": [
        "**3.3 The final matric display - F1 macro after marging labels**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-bgz8WeKpEyT",
      "metadata": {
        "id": "-bgz8WeKpEyT"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file\n",
        "df = pd.read_excel('Labeled.xlsx', sheet_name='Test', engine='openpyxl')\n",
        "\n",
        "columns_range = df.columns[34:70]\n",
        "# Extract the first row values from the specified columns\n",
        "num_rows = 211\n",
        "\n",
        "# Replace 'no' with 'cannot be inferred' and 'yes' with 'plausibly' in the specified columns and rows\n",
        "df.loc[:num_rows-1, columns_range] = df.loc[:num_rows-1, columns_range].replace({'no': 0, 'cannot be inferred': 0,'irrelevant':0, 'yes': 1, 'plausibly': 1})\n",
        "results = []\n",
        "num_rows = 100\n",
        "# Iterate over specified columns\n",
        "for j in range(47):\n",
        "    column_name = df.columns[23 + j]\n",
        "    if column_name not in ['age_female', 'age_male', 'author_gender']:\n",
        "        Predicted_values = df.iloc[:num_rows][column_name].tolist()\n",
        "        True_values = df.iloc[109:109 + num_rows][column_name].tolist()\n",
        "        results.append({\n",
        "            'Risk Factor': column_name,\n",
        "            'F1 Score - macro': round(f1_score(True_values, Predicted_values, average='macro'), 2),\n",
        "        })\n",
        "results_df = pd.DataFrame(results).sort_values(by='F1 Score - macro', ascending=False)\n",
        "\n",
        "print(tabulate(results_df, headers='keys', tablefmt='pretty', showindex=False))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DerAwQH4uHvQ",
      "metadata": {
        "id": "DerAwQH4uHvQ"
      },
      "source": [
        "**3.4 Keep the reliable risk factors (>0.9) for the models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zOJhnS5xqpso",
      "metadata": {
        "id": "zOJhnS5xqpso"
      },
      "outputs": [],
      "source": [
        "# Filter for rows with F1 Score - macro >= 0.9\n",
        "high_f1_scores = results_df[results_df['F1 Score - macro'] >= 0.9]\n",
        "\n",
        "# Exclude the 'author_role' column\n",
        "high_f1_scores_filtered = high_f1_scores[high_f1_scores['Risk Factor'] != 'author_role']\n",
        "\n",
        "# Create a list of risk factors with F1 scores above 0.9, excluding 'author_role'\n",
        "high_f1_risk_factors_list = high_f1_scores_filtered['Risk Factor'].tolist()\n",
        "\n",
        "# Output the list\n",
        "high_f1_risk_factors_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dZN3s3cC86V",
      "metadata": {
        "id": "6dZN3s3cC86V"
      },
      "source": [
        "Corr of selected columns with new labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NSposmsY_O1E",
      "metadata": {
        "id": "NSposmsY_O1E"
      },
      "source": [
        "# **Part 4 - Baselines**\n",
        "We predicted the risk factor: physical violence.\n",
        "\n",
        "Goals:\n",
        "* Establish a baseline for score.\n",
        "* Data Insights: Identify key features and important words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8E8WfH6EhQ_H",
      "metadata": {
        "id": "8E8WfH6EhQ_H"
      },
      "source": [
        "**4.1 Logistic regression**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculated correlation"
      ],
      "metadata": {
        "id": "LcWw9eNmIYlO"
      },
      "id": "LcWw9eNmIYlO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ExQ1jlQAC8ql",
      "metadata": {
        "id": "ExQ1jlQAC8ql"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file\n",
        "df_all = pd.read_excel('Labeled.xlsx', sheet_name='All', engine='openpyxl')\n",
        "\n",
        "# Extract the columns corresponding to the top column names and merge labels\n",
        "df_top_columns = df_all[high_f1_risk_factors_list].replace({'no': 0, 'cannot be inferred': 0,'irrelevant':0, 'yes': 1, 'plausibly': 1})\n",
        "# Create a correlation matrix between these columns\n",
        "correlation_matrix = df_top_columns.corr()\n",
        "# Set up the matplotlib figure with a larger size and adjust font size\n",
        "plt.figure(figsize=(22, 20))\n",
        "sns.set(font_scale=1.2)\n",
        "# Draw the heatmap without the mask and correct aspect ratio\n",
        "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, fmt='.1f', linewidths=0.5)\n",
        "# Rotate the x and y axis labels for better readability\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
        "plt.yticks(rotation=0, fontsize=12)\n",
        "\n",
        "# Add a title to the heatmap\n",
        "plt.title('Correlation Matrix for Reliable Risk Factors', fontsize=18)\n",
        "\n",
        "# Adjust the layout to fit everything within the figure area\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot as an image and display it\n",
        "plt.savefig('correlation_matrix.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30uHKCknBdpK",
      "metadata": {
        "id": "30uHKCknBdpK"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file\n",
        "df_all = pd.read_excel('Labeled.xlsx', sheet_name='All', engine='openpyxl')\n",
        "\n",
        "# Replace specific values in the entire DataFrame\n",
        "replacement_dict = {\n",
        "    'no': 0,\n",
        "    'cannot be inferred': 0,\n",
        "    'irrelevant': 0,\n",
        "    'yes': 1,\n",
        "    'plausibly': 1\n",
        "}\n",
        "df_all = df_all[high_f1_risk_factors_list].replace(replacement_dict)\n",
        "\n",
        "# List of target columns to predict\n",
        "#Target_to_predict = ['physical_violence', 'aggressive_behavior', 'gaslighting', 'narcissistic_traits']\n",
        "Target_to_predict = ['physical_violence']\n",
        "\n",
        "# Iterate over each target column\n",
        "for target_column in Target_to_predict:\n",
        "    print(f\"\\nThe target risk factor is: {target_column}\")\n",
        "\n",
        "    # Separate features and target variable\n",
        "    X = df_all.drop(columns=[target_column])\n",
        "    y = df_all[target_column]\n",
        "\n",
        "    # Ensure the target variable is binary\n",
        "    if y.nunique() != 2:\n",
        "        print(f\"Skipping '{target_column}' as it is not binary.\")\n",
        "        continue\n",
        "\n",
        "    # Add a constant to the features (intercept)\n",
        "    X = sm.add_constant(X)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Initialize the logistic regression model using statsmodels\n",
        "    model = sm.Logit(y_train, X_train)\n",
        "\n",
        "    # Fit the model\n",
        "    result = model.fit()\n",
        "\n",
        "    # Print the summary which includes coefficients, z-values, and p-values\n",
        "    print(result.summary2())\n",
        "\n",
        "    # Remove columns with p-value > 0.05\n",
        "    significant_columns = result.pvalues[result.pvalues <= 0.05].index.tolist()\n",
        "    if 'const' in significant_columns:\n",
        "        significant_columns.remove('const')\n",
        "\n",
        "    # Print the significant columns\n",
        "    print(f\"Significant columns for {target_column}: {significant_columns}\")\n",
        "\n",
        "    # Recreate the feature set with only significant columns\n",
        "    X_significant = X[significant_columns]\n",
        "    X_significant = sm.add_constant(X_significant)\n",
        "\n",
        "    # Split the data again with significant columns\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_significant, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Initialize the logistic regression model using statsmodels with significant columns\n",
        "    model_significant = sm.Logit(y_train, X_train)\n",
        "\n",
        "    # Fit the model with significant columns\n",
        "    result_significant = model_significant.fit()\n",
        "\n",
        "    # Print the summary for the model with significant columns\n",
        "    print(result_significant.summary2())\n",
        "\n",
        "    # Make predictions on the test set with significant columns\n",
        "    y_pred_significant = result_significant.predict(X_test)\n",
        "    y_pred_binary_significant = [1 if x > 0.5 else 0 for x in y_pred_significant]\n",
        "\n",
        "    # Evaluate the model with significant columns\n",
        "    accuracy_significant = accuracy_score(y_test, y_pred_binary_significant)\n",
        "    report_significant = classification_report(y_test, y_pred_binary_significant, target_names=['0', '1'])\n",
        "    print(f\"\\n The target risk factor is: {target_column}\")\n",
        "    print(report_significant)\n",
        "    print(f\"Accuracy with significant columns: {accuracy_significant:.2f}\")\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4veiUwXN6Yic",
      "metadata": {
        "id": "4veiUwXN6Yic"
      },
      "source": [
        "**4.2 TF-IDF + SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7XXPuGBrDkBp",
      "metadata": {
        "id": "7XXPuGBrDkBp"
      },
      "outputs": [],
      "source": [
        "# read the excel file\n",
        "df = pd.read_excel('Labeled.xlsx', sheet_name='All', engine='openpyxl')\n",
        "\n",
        "# keep only relevant columns (columns that are interesting to predict and Yoni got high f1 score on them)\n",
        "feature_column = ['post_body']\n",
        "abusive_types_columns = ['physical_violence','sexual_violence']\n",
        "risk_factors_columns = ['social_isolation', 'gaslighting', 'mental_condition', 'daily_activity_control', 'aggressive_behavior', 'narcissistic_traits']\n",
        "\n",
        "# labels_columns = ['abusive_relationship','emotional_violence',\n",
        "#                     'physical_violence','sexual_violence','economic_violence']\n",
        "df = df[feature_column + abusive_types_columns + risk_factors_columns]\n",
        "\n",
        "# re-label the data and unite labels - ('no' + 'cannot be inferred' + 'irrelevant')  and  ('yes' + 'plausibly')\n",
        "converts_dict = {\n",
        "    'no': 0,\n",
        "    'cannot be inferred': 0,\n",
        "    'irrelevant': 0,\n",
        "    'yes': 1,\n",
        "    'plausibly': 1\n",
        "}\n",
        "\n",
        "df = df.replace(converts_dict)\n",
        "\n",
        "# preprocess the 'post_body' column - convert to TF-IDF vector representation\n",
        "df['post_body'] = df['post_body'].str.lower()\n",
        "\n",
        "# Calculate Word Frequencies\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(df['post_body'])\n",
        "\n",
        "# Sum up the counts of each vocabulary word\n",
        "word_counts = X.toarray().sum(axis=0)\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Count the number of unique words\n",
        "vocabulary_size = len(vocab)\n",
        "print(f'There are {vocabulary_size} different words in the vocabulary')\n",
        "\n",
        "# Create a dictionary with word frequencies\n",
        "word_freq = dict(zip(vocab, word_counts))\n",
        "\n",
        "# Filter Words by Frequency Threshold=200\n",
        "frequency_threshold = 200\n",
        "filtered_words = {word: freq for word, freq in word_freq.items() if freq >= frequency_threshold}\n",
        "\n",
        "print(len(filtered_words))\n",
        "\n",
        "# Set max_features Based on Filtered Words\n",
        "max_features = len(filtered_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nl-jE1GzkMoC",
      "metadata": {
        "id": "nl-jE1GzkMoC"
      },
      "outputs": [],
      "source": [
        "# max_features is the embedding dim\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "post_body_vectors = vectorizer.fit_transform(df['post_body'])\n",
        "\n",
        "\n",
        "X_train_dict = {}\n",
        "X_test_dict = {}\n",
        "y_train_dict = {}\n",
        "y_test_dict = {}\n",
        "\n",
        "# Perform stratified train-test split for each label\n",
        "labels_columns = abusive_types_columns + risk_factors_columns\n",
        "for label in labels_columns:\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "    train_indices, test_indices = next(sss.split(post_body_vectors, df[label]))\n",
        "    X_train_dict[label], X_test_dict[label] = post_body_vectors[train_indices], post_body_vectors[test_indices]\n",
        "    y_train_dict[label], y_test_dict[label] = df[label].iloc[train_indices].values, df[label].iloc[test_indices].values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mIv6KonlQOQa",
      "metadata": {
        "id": "mIv6KonlQOQa"
      },
      "source": [
        "**Plotting the data distribution in a 2D space**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_rKbsF2IFu5a",
      "metadata": {
        "id": "_rKbsF2IFu5a"
      },
      "outputs": [],
      "source": [
        "# perform t-SNE to reduce to 2D\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_vectors = tsne.fit_transform(post_body_vectors.toarray())\n",
        "\n",
        "plot_df = pd.DataFrame({\n",
        "    'x': reduced_vectors[:, 0],\n",
        "    'y': reduced_vectors[:, 1],\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OhBcmWxSUdYL",
      "metadata": {
        "id": "OhBcmWxSUdYL"
      },
      "outputs": [],
      "source": [
        "for column in labels_columns:\n",
        "    plot_df['label'] = df[column].map({1: 'yes', 0: 'no'})\n",
        "\n",
        "    # plot the reduced vectors using Plotly\n",
        "    fig = px.scatter(\n",
        "      plot_df, x='x', y='y', color=plot_df['label'].astype(str),\n",
        "      title=f'2D representation of TF-IDF vectors, classified by {column}',\n",
        "      labels={'color': 'Label'},\n",
        "      width=800, height=600\n",
        "    )\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J4sKLfpwSQuR",
      "metadata": {
        "id": "J4sKLfpwSQuR"
      },
      "source": [
        "**Creating the SVM classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bUxvRNRPINh5",
      "metadata": {
        "id": "bUxvRNRPINh5"
      },
      "outputs": [],
      "source": [
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "f1_per_kernel = {kernel: {} for kernel in kernels}\n",
        "\n",
        "for kernel in kernels:\n",
        "    svm_classifier = SVC(kernel=kernel, C=1.0, random_state=42, class_weight='balanced')\n",
        "\n",
        "    f1_scores = {label: [] for label in labels_columns}\n",
        "\n",
        "    for column in tqdm(labels_columns, desc=f\"Processing label columns for {kernel} kernel\"):\n",
        "        svm_classifier.fit(X_train_dict[column], y_train_dict[column])\n",
        "        y_pred = svm_classifier.predict(X_test_dict[column])\n",
        "        f1 = round(f1_score(y_test_dict[column], y_pred, average='macro'), 4)\n",
        "        f1_scores[column].append(f1)\n",
        "\n",
        "    f1_per_kernel[kernel] = f1_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V2QEG57IY5Q9",
      "metadata": {
        "id": "V2QEG57IY5Q9"
      },
      "source": [
        "**Adding plots for the different f1 scores per label and kernel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3pfaNb81SV1A",
      "metadata": {
        "id": "3pfaNb81SV1A"
      },
      "outputs": [],
      "source": [
        "def plot_f1_scores(column_list, title):\n",
        "    # Prepare data for plotting\n",
        "    plot_data = {label: [f1_per_kernel[kernel][label][0] for kernel in kernels] for label in column_list}\n",
        "    df_plot = pd.DataFrame(plot_data, index=kernels)\n",
        "\n",
        "    # Plot the F1 scores\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    bar_width = 0.2\n",
        "    index = np.arange(len(column_list))\n",
        "\n",
        "    # Generate shades of green using a colormap\n",
        "    colormap = cm.get_cmap('Greens', len(kernels) + 3)\n",
        "    green_shades = [colormap(i + 1) for i in range(len(kernels))]\n",
        "\n",
        "    for i, kernel in enumerate(kernels):\n",
        "        ax.bar(index + i * bar_width, df_plot.loc[kernel], bar_width, color=green_shades[i], label=kernel)\n",
        "\n",
        "    ax.set_xlabel('Label Columns')\n",
        "    ax.set_ylabel('F1 Score')\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(index + bar_width * (len(kernels) - 1) / 2)\n",
        "    ax.set_xticklabels(column_list)\n",
        "\n",
        "    # Position the legend outside the plot area\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for abusive_types_columns\n",
        "plot_f1_scores(abusive_types_columns, 'F1 Scores for Abusive Types Columns by Kernel')\n",
        "\n",
        "# Plot for risk_factors_columns\n",
        "plot_f1_scores(risk_factors_columns, 'F1 Scores for Risk Factors Columns by Kernel')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2TuTLARF7myv",
      "metadata": {
        "id": "2TuTLARF7myv"
      },
      "source": [
        "**Getting TF-IDF statistics - the scores of the words**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RusejnAEzuhH",
      "metadata": {
        "id": "RusejnAEzuhH"
      },
      "outputs": [],
      "source": [
        "nltk.download('words')\n",
        "\n",
        "# here I removed all the non-English words. This might harm our reliability, but it can make the results more interpretable\n",
        "english_words = set(words.words())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6p5waen4B0Iz",
      "metadata": {
        "id": "6p5waen4B0Iz"
      },
      "outputs": [],
      "source": [
        "# initial the possible labels again, we might want different features for this part\n",
        "labels_columns = abusive_types_columns\n",
        "terms_score_dict = {label: None for label in labels_columns}\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "\n",
        "for label in labels_columns:\n",
        "    # keeping posts that are positive ('yes') for the specific label\n",
        "    positive_on_label_posts = df['post_body'][df[label] == 1].str.lower()\n",
        "\n",
        "    tfidf_matrix = vectorizer.fit_transform(positive_on_label_posts)\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "    term_scores = defaultdict(float)\n",
        "    term_counts = defaultdict(int)\n",
        "\n",
        "    for row in tfidf_matrix.toarray():\n",
        "        for term_idx, score in enumerate(row):\n",
        "            if score > 0:\n",
        "                term = terms[term_idx]\n",
        "                term_scores[term] += score\n",
        "                term_counts[term] += 1\n",
        "\n",
        "    mean_term_scores = {term: round(term_scores[term] / term_counts[term],3) for term in term_scores}\n",
        "\n",
        "    # Filter out non-real words\n",
        "    mean_term_scores = {term: score for term, score in mean_term_scores.items() if term in english_words}\n",
        "\n",
        "    term_score_df = pd.DataFrame(list(mean_term_scores.items()), columns=['Term', 'Mean TF-IDF Score'])\n",
        "\n",
        "    terms_score_dict[label] = term_score_df.sort_values(by='Mean TF-IDF Score', ascending=False)\n",
        "\n",
        "\n",
        "for label in terms_score_dict:\n",
        "    print(f\"Top 10 terms for {label}:\")\n",
        "    print(terms_score_dict[label][['Term', 'Mean TF-IDF Score']].head(10))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8o-RPsfc5Jqu",
      "metadata": {
        "id": "8o-RPsfc5Jqu"
      },
      "outputs": [],
      "source": [
        "# extracting the most significant features from the tf-ids vectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "idf_scores = vectorizer.idf_\n",
        "\n",
        "idf_df = pd.DataFrame({'term': feature_names, 'score': idf_scores})\n",
        "\n",
        "print(idf_df.sort_values(by='score', ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W5BqsnjTA74i",
      "metadata": {
        "id": "W5BqsnjTA74i"
      },
      "source": [
        "# **Part 5 - LIME analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T2N6Y5H8DBgU",
      "metadata": {
        "id": "T2N6Y5H8DBgU"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"distilbert_model.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7d75e10b1f3ab99",
      "metadata": {
        "id": "a7d75e10b1f3ab99"
      },
      "source": [
        "**0. Hyper-parameters and models**\n",
        "\n",
        "Here you need to select the model you want to use. Modify this cell before using the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4598d206da02b11",
      "metadata": {
        "id": "4598d206da02b11"
      },
      "outputs": [],
      "source": [
        "# Initiating the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Insert the path to the xlsx file and the name of the relevant sheet\n",
        "file_path = '../content/Abusive Relationship Stories.xlsx'\n",
        "sheet_name = 'Abusive Relationship Stories'\n",
        "\n",
        "### If using RoBerta ###\n",
        "##tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "##model = RobertaForSequenceClassification.from_pretrained('model', output_attentions=True, num_labels=1)\n",
        "\n",
        "### If using DistilBERT ###\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert_model',  output_attentions=True, num_labels=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "558fc1b96eefa5c7",
      "metadata": {
        "id": "558fc1b96eefa5c7"
      },
      "source": [
        "**1. Load Dataset and Preprocessing**\n",
        "\n",
        "We will be focusing on classifying the posts to positive and negative physical violence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5458ddb41a09baa2",
      "metadata": {
        "id": "5458ddb41a09baa2"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data and select columns\n",
        "df = pd.read_excel('Labeled.xlsx', sheet_name='Test', engine='openpyxl')\n",
        "df = df[['title', 'body', 'physical_violence']]\n",
        "\n",
        "# Replace target labels\n",
        "df['physical_violence'] = df['physical_violence'].map({\n",
        "    'no': 0,\n",
        "    'cannot be inferred': 0,\n",
        "    'irrelevant': 0,\n",
        "    'yes': 1,\n",
        "    'plausibly': 1\n",
        "})\n",
        "\n",
        "# Rename columns\n",
        "df.columns = ['title', 'body', 'label']\n",
        "\n",
        "# Remove missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Convert to string\n",
        "df['title'] = df['title'].astype(str)\n",
        "df['body'] = df['body'].astype(str)\n",
        "\n",
        "# Plot the label distribution\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e29eaa01989fe868",
      "metadata": {
        "id": "e29eaa01989fe868"
      },
      "source": [
        "**2. Tokenization and Length Distribution Analysis**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c7a98ea2cf03d",
      "metadata": {
        "id": "c3c7a98ea2cf03d"
      },
      "outputs": [],
      "source": [
        "# Tokenize the texts and calculate lengths (in tokens)\n",
        "token_lengths = [len(tokenizer.encode(text, truncation=False)) for text in df['body']]\n",
        "\n",
        "# Count how many texts are longer than 512 tokens (the limit of the tokenizer)\n",
        "long_texts_count = sum(1 for length in token_lengths if length > 512)\n",
        "long_texts_percentage = round(100 * long_texts_count / len(df['body']), 3)\n",
        "\n",
        "# Print the number of texts longer than 512 tokens with percentage\n",
        "print(f'Number of texts longer than 512 tokens: {long_texts_count}, which are {long_texts_percentage}% of the data.')\n",
        "\n",
        "# Plot token length distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(token_lengths, bins=50, alpha=0.7, label='Post body', color='green')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Text Token Length Distribution (Before Summarization)')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37f6ff4a0b847c84",
      "metadata": {
        "id": "37f6ff4a0b847c84"
      },
      "source": [
        "**3. Filter Texts by Token Length**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b44ad57e88f4d3",
      "metadata": {
        "id": "e6b44ad57e88f4d3"
      },
      "outputs": [],
      "source": [
        "# Filter the dataset to only include rows where the length of the text is less than 512 tokens (or 700, then truncate to 512)\n",
        "filtered_df = df[[len(tokenizer.encode(text, truncation=False)) < 1000 for text in df['body']]]\n",
        "\n",
        "# Show the number of rows in the new filtered dataset\n",
        "print(f'Number of texts with less than 1000 tokens: {len(filtered_df)}')\n",
        "\n",
        "# Recalculate the token lengths for the filtered dataset\n",
        "filtered_token_lengths = [len(tokenizer.encode(text, truncation=False)) for text in filtered_df['body']]\n",
        "\n",
        "# Plot token length distribution for the filtered dataset\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(filtered_token_lengths, bins=50, alpha=0.7, label='Post body (filtered)', color='green')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Text Token Length Distribution (Filtered, <700 tokens)')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Check the effect of the data filtering on the label distribution\n",
        "sns.countplot(x='label', data=filtered_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d66ad7de8b00918d",
      "metadata": {
        "id": "d66ad7de8b00918d"
      },
      "source": [
        "As we can see, dropping the long posts (more than 700 tokens) helps a bit to balance the labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a2cbd3d6a0ebd73",
      "metadata": {
        "id": "4a2cbd3d6a0ebd73"
      },
      "source": [
        "**4. Hyperparameters and Data Split**\n",
        "* We will use the train set solely for training the model.\n",
        "* We will use the test sets both as validations set in training, and to analyze the model with LIME.\n",
        "\n",
        "**Note:** The GPU we used had low vRAM, so we limited the size of samples in the test set to be up to 150 tokens (will be later used during LIME). If you have a bigger GPU, don't run this cell. Instead, activate the next cell for regular train-test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43220323b076ddc3",
      "metadata": {
        "id": "43220323b076ddc3"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "hyperparameters = {\n",
        "    'batch_size': 16,\n",
        "    'epochs': 2,\n",
        "    'tokenizer_max_length': 512,  # Max length for training samples\n",
        "    'test_max_length': 512,       # Max length for test samples\n",
        "    'class_1_weight': 3.0,\n",
        "    'learning_rate': 1e-5,\n",
        "    'test_size': 0.2\n",
        "}\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Filter the dataset to separate samples that fit the test max length\n",
        "filtered_texts = filtered_df['body'].tolist()\n",
        "filtered_labels = filtered_df['label'].tolist()\n",
        "\n",
        "short_texts, short_labels = [], []\n",
        "long_texts, long_labels = [], []\n",
        "\n",
        "for text, label in zip(filtered_texts, filtered_labels):\n",
        "    encoding = tokenizer(text, truncation=True, max_length=hyperparameters['tokenizer_max_length'], return_tensors='pt')\n",
        "    if encoding['input_ids'].shape[1] <= hyperparameters['test_max_length']:\n",
        "        short_texts.append(text)\n",
        "        short_labels.append(label)\n",
        "    else:\n",
        "        long_texts.append(text)\n",
        "        long_labels.append(label)\n",
        "\n",
        "# Determine the number of samples needed for the test set (20% of the data)\n",
        "test_size = int(hyperparameters['test_size'] * len(filtered_texts))\n",
        "actual_test_size = min(test_size, len(short_texts))\n",
        "\n",
        "# Randomly sample from short texts for the test set\n",
        "test_indices = random.sample(range(len(short_texts)), actual_test_size)\n",
        "test_texts = [short_texts[i] for i in test_indices]\n",
        "test_labels = [short_labels[i] for i in test_indices]\n",
        "\n",
        "# Use remaining samples for training\n",
        "train_texts = long_texts + [short_texts[i] for i in range(len(short_texts)) if i not in test_indices]\n",
        "train_labels = long_labels + [short_labels[i] for i in range(len(short_labels)) if i not in test_indices]\n",
        "\n",
        "# Display statistics\n",
        "print(f'train_size: {len(train_texts)}')\n",
        "print(f'train 0 label count: {train_labels.count(0)}')\n",
        "print(f'train 1 label count: {train_labels.count(1)}')\n",
        "print()\n",
        "print(f'test_size: {len(test_texts)}')\n",
        "print(f'test 0 label count: {test_labels.count(0)}')\n",
        "print(f'test 1 label count: {test_labels.count(1)}')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a646c765856c3b3",
      "metadata": {
        "id": "8a646c765856c3b3"
      },
      "source": [
        "**Activate this cell to run a regular train-test split**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8592198db19316",
      "metadata": {
        "id": "6f8592198db19316"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Define hyperparameters\n",
        " hyperparameters = {\n",
        "     'batch_size': 16,\n",
        "     'epochs': 2,\n",
        "     'tokenizer_max_length': 512,\n",
        "     'class_1_weight': 3.0,\n",
        "     'learning_rate': 1e-5\n",
        " }\n",
        "\n",
        " # Split the filtered data into training and testing sets\n",
        " train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "     filtered_df['body'].tolist(),\n",
        "     filtered_df['label'].tolist(),\n",
        "     test_size=0.2,\n",
        "     random_state=42\n",
        " )\n",
        "\n",
        " print(f'train_size: {len(train_texts)}')\n",
        " print(f'train 0 label count: {train_labels.count(0)}')\n",
        " print(f'train 1 label count: {train_labels.count(1)}')\n",
        " print()\n",
        "\n",
        " print(f'test_size: {len(test_texts)}')\n",
        " print(f'test 0 label count: {test_labels.count(0)}')\n",
        " print(f'test 1 label count: {test_labels.count(1)}')\n",
        " print()\n",
        "\"\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afbdce3503e0c8cc",
      "metadata": {
        "id": "afbdce3503e0c8cc"
      },
      "source": [
        "**5. Tokenization and DataLoader Setup**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f499f5f24a6d7af",
      "metadata": {
        "id": "9f499f5f24a6d7af"
      },
      "outputs": [],
      "source": [
        "# Tokenization function\n",
        "def tokenize_data(texts, labels,tokenizer):\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=hyperparameters['tokenizer_max_length'])\n",
        "    inputs = torch.tensor(encodings['input_ids'])\n",
        "    attention_masks = torch.tensor(encodings['attention_mask'])\n",
        "    labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
        "    return TensorDataset(inputs, attention_masks, labels)\n",
        "\n",
        "# Tokenize the data\n",
        "train_dataset = tokenize_data(train_texts, train_labels,tokenizer)\n",
        "test_dataset = tokenize_data(test_texts, test_labels,tokenizer)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=hyperparameters['batch_size'])\n",
        "\n",
        "# Initiating the device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86f5f9fd4232d5cf",
      "metadata": {
        "id": "86f5f9fd4232d5cf"
      },
      "source": [
        "**6. Model Training and Evaluations, and saving**\n",
        "\n",
        "(Activating the training is not in this cell, but in the next one)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "393179f9921413f8",
      "metadata": {
        "id": "393179f9921413f8"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, test_loader, device, epochs=hyperparameters['epochs']):\n",
        "    model.to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    pos_weight = torch.tensor([hyperparameters['class_1_weight']]).to(device)\n",
        "    loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = AdamW(model.parameters(), lr=hyperparameters['learning_rate'])\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = loss_fn(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            progress_bar.set_postfix({'Batch Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "        # Evaluate on test set after each epoch\n",
        "        evaluate_model(model, test_loader,loss_fn)\n",
        "\n",
        "    # Move model back to CPU after training\n",
        "    model.to('cpu')\n",
        "\n",
        "\n",
        "# Model evaluation function\n",
        "def evaluate_model(model, val_loader, loss_fn):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = loss_fn(logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Apply sigmoid activation and threshold to get binary predictions\n",
        "            preds = torch.round(torch.sigmoid(logits))\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    # Calculate Precision, Recall, F1\n",
        "    precision = precision_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af551219d395ff55",
      "metadata": {
        "id": "af551219d395ff55"
      },
      "source": [
        "#### Activate the two cells below to activate the model training and save it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e46389157442c0",
      "metadata": {
        "id": "91e46389157442c0"
      },
      "outputs": [],
      "source": [
        "# train_model(model, train_loader, test_loader, device)\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc844a8c3bfa473",
      "metadata": {
        "id": "2dc844a8c3bfa473"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "import os\n",
        "\n",
        " # Save model and tokenizer\n",
        " output_dir = 'distilbert_model'\n",
        " if not os.path.exists(output_dir):\n",
        "     os.makedirs(output_dir)\n",
        "\n",
        " model.save_pretrained(output_dir)\n",
        " tokenizer.save_pretrained(output_dir)\n",
        "\n",
        " print(f\"Model and tokenizer saved to {output_dir}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6464b3a52bb8f670",
      "metadata": {
        "id": "6464b3a52bb8f670"
      },
      "source": [
        "**7. Preparing the test set dataframe for LIME analysis**\n",
        "\n",
        "Splitting the test_set into eight groups, based on the accuracy and confidence:\n",
        "* True Positive (TP) with High Confidence.\n",
        "* True Positive (TP) with Low Confidence.\n",
        "* True Negative (TN) with High Confidence.\n",
        "* True Negative (TN) with Low Confidence.\n",
        "* False Positive (FP) with High Confidence.\n",
        "* False Positive (FP) with Low Confidence.\n",
        "* False Negative (FN) with High Confidence.\n",
        "* False Negative (FN) with Low Confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98da0f15ecf36908",
      "metadata": {
        "id": "98da0f15ecf36908"
      },
      "outputs": [],
      "source": [
        "# Prediction function (using probabilities for confidence)\n",
        "def predict_with_confidence(model, dataloader):\n",
        "    model.to(device)  # Move model to GPU\n",
        "    model.eval()\n",
        "    predictions, confidences_class_1, true_labels = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Use tqdm to add a progress bar to the dataloader loop\n",
        "        for batch in tqdm(dataloader, desc=\"Predicting\", leave=False):\n",
        "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()  # Sigmoid output, gives probability for class 1\n",
        "            preds = np.round(probs)\n",
        "\n",
        "            predictions.extend(preds.flatten())  # Flatten predictions\n",
        "            confidences_class_1.extend(probs.flatten())  # Confidence scores for class 1\n",
        "            true_labels.extend(labels.cpu().numpy().flatten())  # Flatten true labels\n",
        "\n",
        "    model.to('cpu')  # Move model back to CPU\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return np.array(predictions), np.array(confidences_class_1), np.array(true_labels)\n",
        "\n",
        "# Define thresholds for high and low confidence\n",
        "high_conf_thresh = 0.8  # High confidence for class 1\n",
        "low_conf_thresh = 0.2   # High confidence for class 0\n",
        "\n",
        "# Make predictions on the test set and get confidence scores\n",
        "predictions, confidences_class_1, true_labels = predict_with_confidence(model, test_loader)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Create a DataFrame to store predictions, true labels, confidence values, and confidence scores\n",
        "df_test = pd.DataFrame({\n",
        "    'body': test_texts,       # Assuming 'test_texts' contains the original text data\n",
        "    'y_true': true_labels,    # True labels\n",
        "    'y_pred': predictions,    # Model predictions\n",
        "    'confidence_class_1': confidences_class_1, # Probability for class 1 (sigmoid output)\n",
        "    'confidence_in_predicted_class': np.where(predictions == 1, confidences_class_1, 1 - confidences_class_1)  # Confidence in the predicted class\n",
        "})\n",
        "\n",
        "# Categorize into True Positive, True Negative, False Positive, False Negative\n",
        "df_test['group'] = np.where((df_test['y_true'] == 1) & (df_test['y_pred'] == 1), 'True Positive',\n",
        "                    np.where((df_test['y_true'] == 0) & (df_test['y_pred'] == 0), 'True Negative',\n",
        "                    np.where((df_test['y_true'] == 0) & (df_test['y_pred'] == 1), 'False Positive', 'False Negative')))\n",
        "\n",
        "# Define high and low confidence\n",
        "df_test['confidence_level'] = np.where((df_test['confidence_in_predicted_class'] >= high_conf_thresh) |\n",
        "                                       (df_test['confidence_in_predicted_class'] <= low_conf_thresh),\n",
        "                                       'High Confidence', 'Low Confidence')\n",
        "\n",
        "# Combine group (TP, TN, FP, FN) and confidence level\n",
        "df_test['final_group'] = df_test['group'] + ' - ' + df_test['confidence_level']\n",
        "\n",
        "# View distribution of the final groups\n",
        "group_counts = df_test['final_group'].value_counts()\n",
        "print(group_counts)\n",
        "\n",
        "# Plot the distribution of the final groups as a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "group_counts.plot(kind='bar', color='skyblue')\n",
        "plt.title('Distribution of Model Predictions by Accuracy and Confidence')\n",
        "plt.xlabel('Group')\n",
        "plt.ylabel('Number of Predictions')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84hkKIgKI6KA",
      "metadata": {
        "id": "84hkKIgKI6KA"
      },
      "source": [
        "Stacked bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hd7EVwZuHNll",
      "metadata": {
        "id": "hd7EVwZuHNll"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Count occurrences of each group and confidence level\n",
        "group_confidence_counts = df_test.groupby(['group', 'confidence_level']).size().unstack(fill_value=0)\n",
        "\n",
        "# Sort groups by total count (sum of high and low confidence)\n",
        "group_confidence_counts = group_confidence_counts.loc[group_confidence_counts.sum(axis=1).sort_values(ascending=False).index]\n",
        "\n",
        "# Plot the stacked bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = group_confidence_counts.plot(kind='bar', stacked=True, color=['skyblue', 'salmon'])\n",
        "\n",
        "# Add a custom legend with confidence ranges\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles, ['High Confidence [0,0.2]∧[0.8,1]', 'Low Confidence (0.2,0.8)'], title='Confidence Level', loc='upper right')\n",
        "\n",
        "# Add annotations for each segment\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, label_type='center', fontsize=10)\n",
        "\n",
        "# Customize plot appearance\n",
        "plt.title('Distribution of Model Predictions by Accuracy and Confidence')\n",
        "plt.xlabel('Group')\n",
        "plt.ylabel('Number of Predictions')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1a8a48bba26b10",
      "metadata": {
        "id": "aa1a8a48bba26b10"
      },
      "source": [
        "### Column Descriptions for df_test\n",
        "\n",
        "- **body**: The text content of each sample.\n",
        "- **y_true**: The actual label (ground truth) for each sample.\n",
        "- **y_pred**: The predicted label from the model for each sample.\n",
        "- **confidence_class_1**: Model’s confidence score for classifying the sample as Class 1 (abusive).\n",
        "- **confidence_in_predicted_class**: Model's confidence score for the predicted label.\n",
        "- **group**: Category based on the correctness and confidence of the prediction (TP, TN, FP, FN).\n",
        "- **confidence_level**: Confidence level categorization (e.g., Low, High).\n",
        "- **final_group**: Combined 'group' and 'confidence_level.'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ccdb3cca0e26f9b",
      "metadata": {
        "id": "7ccdb3cca0e26f9b"
      },
      "source": [
        "**8. LIME Analysis**\n",
        "### Watch this video to understand LIME:\n",
        "* https://www.youtube.com/watch?v=qQvC6FWlc-E\n",
        "\n",
        "*Note:* From now on, we will conduct various analyses on the model to understand how it makes predictions. This will enable us to improve it with data that reflects our insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e51ce0b63f6de84a",
      "metadata": {
        "id": "e51ce0b63f6de84a"
      },
      "source": [
        "**8.1. Simple LIME examples on dummy samples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa5442b6cc6708f",
      "metadata": {
        "id": "aaa5442b6cc6708f"
      },
      "outputs": [],
      "source": [
        "# Initialize the LIME explainer with fixed class names\n",
        "explainer = LimeTextExplainer(class_names=['Non-Abusive', 'Abusive'])\n",
        "\n",
        "# Efficient Prediction Function for LIME\n",
        "def predict_fn(text, model, tokenizer, max_tokens_length):\n",
        "    encodings = tokenizer(text, truncation=True, padding=True, max_length=max_tokens_length, return_tensors=\"pt\")\n",
        "    input_ids = encodings['input_ids'].to(device)\n",
        "    attention_mask = encodings['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prob = torch.sigmoid(model(input_ids, attention_mask=attention_mask).logits).cpu().numpy()\n",
        "\n",
        "    # Clear GPU memory\n",
        "    del input_ids, attention_mask\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return np.hstack([1 - prob, prob])\n",
        "\n",
        "# --- Per-Sample LIME Analysis (Optimized for GPU Memory) ---\n",
        "def lime_analysis_per_sample(model, tokenizer, categorized_texts, analysis_params):\n",
        "    model.to(device)\n",
        "\n",
        "    for category, texts in categorized_texts.items():\n",
        "        # Display header for the category with bold effect\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"\\033[1m--- LIME Analysis for '{category}' ---\\033[0m\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for text in texts:\n",
        "            # Perform LIME explanation for each sample\n",
        "            explanation = explainer.explain_instance(\n",
        "                text,\n",
        "                lambda x: predict_fn(x, model, tokenizer, analysis_params['max_tokens_length']),\n",
        "                num_features=analysis_params['top_k_features'],\n",
        "                num_samples=analysis_params['num_of_perturbations_in_lime']\n",
        "            )\n",
        "\n",
        "            # Display explanation\n",
        "            explanation.show_in_notebook()  # Viewable in notebooks, or use explanation.as_pyplot_figure() in scripts\n",
        "\n",
        "            # Clear GPU memory after each sample\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    # Move model back to CPU after processing\n",
        "    model.to('cpu')\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "# Dummy sample categorized texts for LIME analysis\n",
        "categorized_texts = {\n",
        "    \"Direct Violence\": [\"He hit him hard.\", \"She punched the wall.\", \"They attacked the stranger.\"],\n",
        "    \"Indirect Violence\": [\"They were shouting loudly.\", \"He threw the chair across the room.\", \"The situation was getting tense.\"],\n",
        "    \"Non-Violent Conflict\": [\"They argued for hours.\", \"He left without saying a word.\", \"She was upset, but kept calm.\"],\n",
        "    \"Neutral\": [\"They went for a walk.\", \"He smiled and waved.\", \"She worked at her desk.\"],\n",
        "    \"Ambiguous\": [\"He pushed him gently.\", \"They were playing rough.\", \"She slapped him jokingly.\"],\n",
        "    \"Sarcasm/Metaphor\": [\"She killed it at the presentation.\", \"He was beating the competition.\"],\n",
        "    \"Threats\": [\"I’ll hurt you if you don’t listen.\", \"She threatened to hit him.\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7486a58f2778b2b3",
      "metadata": {
        "id": "7486a58f2778b2b3"
      },
      "outputs": [],
      "source": [
        "# Define analysis parameters for LIME\n",
        "analysis_params = {\n",
        "    'max_samples_per_category': 20,      # Max samples per category for analysis\n",
        "    'top_k_features': 10,               # Top k features to display in LIME analysis\n",
        "    'max_tokens_length': 200,           # Max token length for the tokenizer\n",
        "    'num_of_perturbations_in_lime': 300 # Number of perturbations for LIME\n",
        "}\n",
        "\n",
        "lime_analysis_per_sample(model, tokenizer, categorized_texts, analysis_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36178ecffa6a038",
      "metadata": {
        "id": "f36178ecffa6a038"
      },
      "source": [
        "**8.2. Low-Confidence Analysis: Identifying Confusing Cases**\n",
        "\n",
        "This analysis focuses on finding cases where the model is most uncertain (confidence level near 0.5).\n",
        "* We will identify the most confusing samples within each category: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
        "\n",
        "\n",
        "* This is a per-sample analysis, aiming to understand the characteristics of samples that the model finds confusing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b69bf3973c4e08",
      "metadata": {
        "id": "71b69bf3973c4e08"
      },
      "outputs": [],
      "source": [
        "# Prepare categorized texts for LIME analysis, selecting only groups that match the condition if specified\n",
        "def select_extreme_samples(df, analysis_params, condition=None):\n",
        "    categorized_texts = {}\n",
        "\n",
        "    for group in df['final_group'].unique():\n",
        "        # Process all groups if condition is None; otherwise, filter based on condition\n",
        "        if condition is None or (condition is not None and condition in group):\n",
        "            group_df = df[df['final_group'] == group].copy()\n",
        "            group_df['distance_from_0.5'] = abs(group_df['confidence_in_predicted_class'] - 0.5)\n",
        "            group_df = group_df.sort_values(by='distance_from_0.5')\n",
        "\n",
        "            # Select up to max_samples_per_category samples\n",
        "            categorized_texts[group] = group_df['body'].tolist()[:min(analysis_params['max_samples_per_category'], len(group_df))]\n",
        "\n",
        "    return categorized_texts\n",
        "\n",
        "\n",
        "# Run the low-confidence analysis\n",
        "categorized_texts = select_extreme_samples(df_test, analysis_params,condition=\"Low Confidence\")\n",
        "lime_analysis_per_sample(model, tokenizer, categorized_texts, analysis_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63e006aeffada12a",
      "metadata": {
        "id": "63e006aeffada12a"
      },
      "source": [
        "**8.3. Strong Tokens Analysis: Key Influential Features by Group**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1199ac4362e70a18",
      "metadata": {
        "id": "1199ac4362e70a18"
      },
      "source": [
        "**Objective:** Identify the most impactful tokens that the model relies on in high-confidence and low-confidence cases across all groups (True Positives, False Positives, True Negatives, and False Negatives).\n",
        "\n",
        "**Purpose:** This analysis helps us uncover patterns in the model’s decision-making by examining words or phrases that strongly influence predictions. Understanding these patterns highlights areas for model improvement.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab052154ee9844f1",
      "metadata": {
        "id": "ab052154ee9844f1"
      },
      "outputs": [],
      "source": [
        "# LIME Analysis Function with model argument in predict_fn\n",
        "def lime_strong_tokens_analysis(model, tokenizer, categorized_texts,analysis_params, num_features=8, num_samples=200):\n",
        "    aggregated_results = {}\n",
        "    model.to(device)\n",
        "\n",
        "    # Define positive and negative direction categories\n",
        "    positive_direction_categories = {\n",
        "       \"False Positive - Low Confidence\"\n",
        "    }\n",
        "\n",
        "    negative_direction_categories = {\n",
        "        \"False Negative - Low Confidence\"\n",
        "    }\n",
        "\n",
        "    # Process each category in categorized_texts\n",
        "    for category, texts in tqdm(categorized_texts.items(), desc=\"Processing Groups\"):\n",
        "        feature_importance = Counter()\n",
        "        all_importances = {}\n",
        "\n",
        "        # Set relevant direction based on category\n",
        "        if category in positive_direction_categories:\n",
        "            relevant_direction = lambda importance: importance > 0\n",
        "        elif category in negative_direction_categories:\n",
        "            relevant_direction = lambda importance: importance < 0\n",
        "        else:\n",
        "            relevant_direction = lambda importance: True  # Default to include all if unspecified\n",
        "\n",
        "        for text in tqdm(texts, desc=f\"Analyzing {category}\", leave=False, dynamic_ncols=True):\n",
        "            explanation = explainer.explain_instance(\n",
        "                text, lambda x: predict_fn(x, model, tokenizer, max_tokens_length=analysis_params['max_tokens_length']),\n",
        "                num_features=analysis_params['top_k_features'], num_samples=analysis_params['num_of_perturbations_in_lime']\n",
        "            )\n",
        "\n",
        "\n",
        "            # Collect and aggregate relevant feature importances\n",
        "            for word, importance in explanation.as_list():\n",
        "                if relevant_direction(importance):\n",
        "                    feature_importance[word] += abs(importance)\n",
        "                    if word not in all_importances:\n",
        "                        all_importances[word] = []\n",
        "                    all_importances[word].append(importance)\n",
        "\n",
        "            # Clear GPU memory after each text\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        # Aggregate top features with mean and standard deviation\n",
        "        top_features = feature_importance.most_common(5) # Change this if you want different number of aggregated features\n",
        "        aggregated_results[category] = [\n",
        "            (word, np.mean(all_importances[word]), np.std(all_importances[word])) for word, _ in top_features\n",
        "        ]\n",
        "\n",
        "    # Reset model to CPU\n",
        "    model.to('cpu')\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return aggregated_results\n",
        "\n",
        "# Function to plot LIME analysis results\n",
        "def plot_lime_results(aggregated_lime_results):\n",
        "    fig, axes = plt.subplots(len(aggregated_lime_results), 1, figsize=(10, len(aggregated_lime_results) * 5))\n",
        "\n",
        "    if len(aggregated_lime_results) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, (group, features) in enumerate(aggregated_lime_results.items()):\n",
        "        ax = axes[idx]\n",
        "        words, means, std_devs = zip(*features)\n",
        "\n",
        "        ax.barh(words, means, xerr=std_devs, color='skyblue' if idx % 2 == 0 else 'salmon', alpha=0.7)\n",
        "        ax.set_title(f'LIME Analysis for \"{group}\" Category', fontsize=14)\n",
        "        ax.set_xlabel('Average Importance of Feature (Across Sentences)', fontsize=12)\n",
        "        ax.invert_yaxis()\n",
        "        ax.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Create categorized texts from extreme samples\n",
        "categorized_body_texts = select_extreme_samples(df_test, analysis_params)\n",
        "\n",
        "# Run LIME analysis and aggregate results\n",
        "aggregated_lime_results = lime_strong_tokens_analysis(model, tokenizer, categorized_body_texts,analysis_params)\n",
        "\n",
        "# Call the plotting function to visualize the results\n",
        "plot_lime_results(aggregated_lime_results)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}